# Server Access Logs Daily Data Pipeline

This project was executed as a part of the [Data Engineering Zoomcamp 2025](https://github.com/DataTalksClub/data-engineering-zoomcamp) course held by [DataTalks.Club](https://datatalks.club/) - a free nine-week course that covers the fundamentals of data engineering. The goal of this project is to apply what was learnt during this course by creating an ELT data pipeline for processing server access logs data:

- <strong>Extract</strong> data from a data source (GitHub repo)
- <strong>Load</strong> the data into a data lake (GCS) and a data warehouse (BigQuery)
- <strong>Transform</strong> the data in the data warehouse using dbt

Finally, the processed data will be presented in a Looker dashboard.

<img src="https://github.com/kkumyk/log_files_analysis_pipeline/blob/master/doc/de-pipeline.drawio.png">

</hr>

##  Project Overview

### Context
Server log files are text files that automatically record events and transactions that happen on a web server, in this case, Heroku. These files help to provide insights into how search engine crawlers and humans navigate a website including such information as what pages they looked at, how people are finding your site, what errors they ran into if any. By analysing log files in real-time, you can identify issues regarding crawl errors, inefficiencies, and quickly make adjustments.

### Expected Output
The output product of this project is a data pipeline that analyses server access logs data generated by [Kabardian Poems Collection](https://kabardian-poems-collection-b906b8b63b33.herokuapp.com/) website hosted on [Heroku](https://www.heroku.com/) server. The ELT data pipeline will extract the data from the server logs, process it by removing unnessesary and sensitive information, upload it to a cloud provider, transform the data to extract the aimed insights and display them in a dashboard. 

## Data Source and Dataset Description

### Dataset
The following data is extracted from the Heroku server logs:
- IP address of the user making the request: <code>10.1.22.113</code>
- Timestamp - time the request was made: <code>2025-02-11 16:04:15.059069+00:00</code>
- HTTP request method: <code>GET</code>
- URL requested: <code>/poems/</code>
- HTTP protocol: <code>HTTP/1.1</code>
- Response code: <code>200</code>
- Size of response: <code>23279</code>
- Referer - the web page the user is visiting from: <code>/poems/64/</code>
- User-Agent - the page that provided the link to make this request, e.g. search engine or social platform: <code>Mozilla/5.0(X11;Linuxx86_64)AppleWebKit/537.36(KHTMLlikeGecko)Chrome/131.0.0.0Safari/537.36</code>

### Data Source
The data is exracted via Heroku CLI using bash commands and uploaded to [GitHub repo](https://github.com/kkumyk/heroku_log_files/releases/tag/daily-upload) similar to how it was done in the course. The data is stored in CSV files and uploaded daily. Each file contains requests from the previous day.

##  Project Architecture and Execution Flow

### Data Flow

The end-to-end data pipeline includes the follwoing steps:
- downloading, processing and uploading of the initial dataset to a data lake;
- moving the data from the lake to a DWH;
- transforming the data in the DWH and preparing it for the dashboard;
- visualising the transformed data.

### Technologies

- Infrastracture: <strong>Terraform</strong> 
- Cloud: <strong>Google Cloud Platform (GCP)</strong> 
    - Data Lake: <strong>Google Cloud Storage (GCS)</strong> 
    - Data Warehouse: <strong>Google Big Query (GBQ)</strong> 
    - Data Visualization: <strong>Looker Studio</strong> 
- Containerization: <strong>Docker Compose</strong> 
- Orchestration: <strong>Kestra</strong> 
- Data Transformation: <strong>Data Build Tool (DBT Cloud)</strong> 
- Programming Language: <strong>Python, SQL</strong>

## Reproducing Project

### 1. Prerequisites
Clone [this repo](https://github.com/kkumyk/log_files_analysis_pipeline).

Make sure you have the following pre-installed components:
- [GCP account](https://cloud.google.com/)
- [Terraform](https://developer.hashicorp.com/terraform/install)
- [Docker](https://docs.docker.com/engine/install/)
  
  <summary>Details below list commands used to install Docker Engine and Docker Compose on Ubuntu 24.04:
  <details>

    ```bash
    # install Docker Engine (used when we want to handle only one container)
    docker --version # Docker version 27.4.1, build b9d17ea

    # verify the installation run
    sudo service docker start

    # this command downloads a test image and runs it in a container
    # when the container runs, it prints a confirmation message and exits
    sudo docker run hello-world

    # install Docker Compose (used when we have multiple containers to handle)
    docker compose version # Docker Compose version v2.32.1
    ```
    </details>
  </summary>

### 2. Google Cloud Platform (GCP) - Project Setup
- Setup up GCP free account if you don't have an account. It expires after 90 days.
- Create a new project and take note of the project number and project ID.
- Create a service account for the project.
- Configure service account to get access to this project and download auth-keys (.json).
- Make sure the service account has all the permissions below:
    - Viewer
    - Storage Admin
    - Storage Object Admin
    - BigQuery Admin
- Download [SDK](https://cloud.google.com/sdk) for local setup.
- Set env var to point to your downloaded GCP auth-key by switching to the folder where the key was saved and running the commands below on the command line:
  ```bash
  export GOOGLE_APPLICATION_CREDENTIALS="<path/to/your/service-account-authkeys>.json"
  
  # Refresh token/session, and verify authentication
  gcloud auth application-default login
  ```
- Enable the following options under the APIs and services section:
  - [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com)
  - [IAM service account credentials API](https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com)

### 3. Create GCP Project Infrastructure with Terraform

Terraform is infrastructure as cloud (IaC) and it will be used to build and destroy GCP resources. The infrastructure we need to create for this project consists of:

- a Cloud Storage Bucket (google_storage-bucket) for our Data Lake
- a BigQuery Dataset (google_bigquery_dataset)

#### Terraform Installation
See full Ubuntu installation instructions [here](https://github.com/kkumyk/data-engineering-zoomcamp/blob/main/1_intro_to_data_engineering/1_README.md#creating-gcp-project-infrastructure-with-terraform).

You will find two files in the cloned [terraform folder](https://github.com/kkumyk/log_files_analysis_pipeline/tree/master/terraform):
  - main.tf
  - variables.tf
    - update with your project ID in <strong>variables.tf</strong>:
      ```tf
      variable "project" {
        description = "ADD-YOUR-PROJECT-ID_HERE"
      }
      ```
After following the commands below you should see "server_logs_bucket" in GCS and "server_logs_data" dataset in BigQuery:

```bash
# Refresh service-account's auth-token for this session:
gcloud auth application-default login

# Initialize configuration and import plugins for Google provider -
# cd to the folder with the Terraform config files and run the following command:
terraform init

# Create resources with Terraform plan; add your project ID when prompted:
terraform plan

# Create Infra with Apply; add your project ID when prompted and type "yes":
terraform apply
```

### 4. Orchestration with Kestra

This section explains how to orchestrate the data ingestion into:
- <strong>a data lake (GCS)</strong> - the files from Github Release will be moved to a <i>server_logs_bucket</i> and 
- <strong>a data warehouse (BigQuery)</strong> - the data from CSV files will be ingested into a table under the newly created <i>server_logs_data</i> dataset.

The cloned kestra folder contains flow files and docker-compose.yaml. Two of them need to be updated, see below:
- docker-compose.yaml : <i>update "your-email-goes-here.com", row 45</i>
  - flows
    - 00_gcp_kv.yaml : <i>update rows 10, 28, 34</i>

      <strong>IMPORTANT! <i>Watch out to NOT submit the updated file to GitHub!</i></strong>
    - 01_gcp_setup.yaml
    - 02_logs_2_gcs_2_bq.yaml

#### Run Docker Compose Container
```bash
# change to the folder that includes your docker-compose.yaml file
cd kestra/

# build docker image
sudo docker compose build

# run docker compose in detached mode
sudo docker compose up -d
```

The above command will spin up two containers:
- kestra-postgres-1
- kestra-kestra-1

Wait untill the above command has finished running, open http://localhost:8080/ in your brouser and add your flows via UI: <i>Flows > Create </i>

Run <i>00_gcp_kv.yaml</i> and <i>01_gcp_setup.yaml</i>.

To run the <i>02_logs_2_gcs_2_bq.yaml</i> file don't press <i>Execute</i> as it contains a trigger.

Go to Triggers > "Backfill executions" > Select the start date > "Execute backfills".

### 5. DBT

####  Prerequisites

Create 2 new empty datasets for your project in BigQuery:
- a development dataset, e.g.: <i>dbt_dev_env</i>
- a production dataset, e.g.: <i>dbt_prod_env</i>

<strong>NOTE:</strong> Make sure you select your region in accordance with the selected region of your entire project.

#### Setting Up dbt Cloud

1. [Create a dbt Cloud account](https://www.getdbt.com/signup) or [log in](https://cloud.getdbt.com/login) into the existing one.  
2. Set up a GitHub repo for your dbt project.
3. Set up dbt Cloud with BigQuery:
    - <strong>connect dbt to BigQuery development <i>dbt_dev_env</i> dataset</strong>:
        - <strong>create a BigQuery service account</strong> - [use instructions here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md#create-a-bigquery-service-account); <strong>NOTE:</strong> The steps for the current service account creation have slightly changed. E.g.: before you can actually create a new service account you will be first asked if you would like to re-use an existing one if such is already there. In this case, press "Continue" to land on a form to set up a new service account by proving your "Service account details":
          - add service account name, e.g.: <i>dbt-service-account</i>
          - press "create and continue"
          - in the "Grant this service account access to project" select "BigQuery Admin"
          - press "Done"
          - select newly created service account and navigate to its "KEYS" section
          - select "create new key" and the key type JSON; this will create and download the key file to your pc.
      - <strong>set up a dbt Cloud project</strong>:
        - Project name: <i>server-logs-daily-data-pipeline</i> and press "Continue"
        - Configure your development environment: Choose Bigquery as your data warehouse
        - Upload the key you downloaded from BQ on the create from file option.
        - Scroll down to the end of the page and set up your development credentials. This can also done under: Develop > Configure Cloud CLI > Credentials > click on project name > Development credentials > Dataset 
        - Setup a repository > GitHub > Connect GitHub Account > Authorize dbt Cloud > Add repository from: Git Clone > paste the SSH key from your repo. [More details here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md#add-github-repository).
        - You will get a deploy key in dbt Cloud. Copy it and head to your GH repo and go to the settings tab. Under security you'll find the menu "D"eploy keys". Click on add key and paste the deploy key provided by dbt cloud. Make sure to tick on "Allow write access".
        - Replicate file contents are in the dbt folder. File structure would look like:
        
        <img src="https://github.com/kkumyk/server-logs-daily-data-pipeline/blob/master/doc/dbt-files-overview.png">

<strong>Deploying with dbt Cloud</strong>

Before going into production, make sure everything is submitted to GitHub. Then:
- navigate to your dbt environments via Deploy > Environments; at this stage only Development environment should be should there
- create a Production environtment: click on the "Create environtment" button on the top right; field that need to be added/selected are:
  - Environment name: Production
  - Environment type: Deployment
  - Connection: BigQuery
  - Dataset: the name of your production dataset in BigQuery and press "Test Connection" and Save after successeful testing.
  - go to Deploy > Jobs
  - click on the "Create job" button, add the following details:
     - Job name: dbt_build
     - Environment: Production
     - Execution settings > Commands:
        - dbt seed
        - dbt run
        - dbt test

There is of course options to schedule the job which I'm not using now as I want manually run the job to see if it simply works in the first place; I therefore save the job and run it by clicking on the "Run now" button.



<!-- 

Connect dbt to your Github repo
1. Connect dbt to BigQuery development dataset and to the Github repo by following [these instructions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md).
2. In the IDE windows, press the green <i>Initilize</i> button to create the project files.
<!-- 3. Inside dbt_project.yml, change the project name both in:
  - the name field and
  - right below the <i>models:</i> block. Cmment or delete the example block at the end. -->


<!-- 

####  Developing <i>log_files_analysis</i> dbt Project

##### Project Setup and Structure

1. After setting up dbt Cloud account, in the Settings of your project rename the default name to "log_files_analysis".
2. Inside <i>dbt_project.yml</i>, change the project name both in:
    - the <i>name</i> field and
    - right below the <i>models</i> block. You may comment or delete the example block at the end.
      ```sql
      name: 'log_files_analysis' 

      models:
        log_files_analysis:
      ```
3. Under the <i>models</i> folder create two sub-folders:
    - <i>core</i>
    - <i>staging</i>

4. The <i>staging</i> folder contains <strong>schema.yml</strong>, which defines:
    - the data sources the models are going to use; here, the partitioned table <i>daily_data</i> from the <i>server_logs_data</i> BigQuery dataset will be used by the <i>stg_daily_data.sql</i> model
    - the schema for the <i>stg_daily_data</i> table that will be created under BigQuery's development dataset based on the <i>stg_daily_data.sql</i> dbt model using the <i>daily_data</i> table.

##### Models

###### 1. stg_daily_data.sql (staging)

<i>stg_daily_data.sql</i> - this model references <i>bot_vs_human.sql</i> macro which separates requests by defining them either as a <i>bot</i> or as a <i>human</i> based on the data found in the <i>user_agent</i> column. E.g.: 
    
    Mozilla/5.0(compatible;YandexBot/3.0;+http://yandex.com/bots)

Run the <i>stg_daily_data.sql</i> model:
```bash
dbt build --select stg_daily_data.sql
```

The result is a <i>stg_daily_data</i> view created under the <i>dbt_dev_env</i> with a column referencing bot vs human flag.


###### 2. page_categories.sql (core) - based on a provided seed data

This module is using a <i>categories_by_page.csv</i> file - supplied wihtin <i>seeds</i> folder - which lists all existing pages on the site. The file has two columns:
  - page url
  - page type: category vs regular page

After adding the CSV file to the seeds folder, run <code>dbt seed</code> command in the dbt command line. Refresh the tab with the BigQuery you will see a new <i>categories_by_page</i> table that will contain the data from the seed file.

The <i>page_categories.sql</i> model uses the seed file and extends it by a <i>cleaned_page_url</i> which contains the same data as in page_url trimmed the trailing slash at the end and the pagination. This version of the page urls will be used in the final dashboard.

Run the <i>page_categories.sql</i> model:
```bash
dbt build --select page_categories.sql
```

###### 3. daily_data_by_page.sql

This model is using previously created models:
- stg_daily_data.sql (staging)
- page_categories.sql (core)

To build your project run:
```bash
dbt build --select daily_data_by_page.sql
```

###### 4. aggregated_data.sql

This model contains the aggregated calculations which will be shown on the dashboard:

  ```bash
  dbt build --select aggregated_data.sql
  ```
<!-- 
  Run dbt build from jobs menu in dbt cloud. -->




### 6. Building Looker Studio Dashboard

When the production models are ready, you can start building a dashboard. The [Server Access Logs Daily Data Pipeline](https://lookerstudio.google.com/u/0/reporting/260145bd-366d-4ab9-93fa-1e6ac5d010f2/page/R9DYE) dashboard is built using Looker Studio. The process for building a dashboard in Looker Studio can be found in [this video](https://www.youtube.com/watch?v=39nLTs74A3E&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=44).



The final dashboard includes the following diagrams:
- total requests
- total requests by page url
- HTTP status codes by page category
- bot vs human requests
- identified bots

<img src="https://github.com/kkumyk/log_files_analysis_pipeline/blob/master/doc/server_access_logs_daily_data_pipeline.png">





<!-- 
1. <strong>Cron</strong>: Three Cron jobs are running daily:
   - Server logs are extracted via Heroku CLI.
   - <i>process_raw_txt_files.sh</i> bash script cleans the collected the data and splits it into CSV files. Each file contains data by day.
   - <i>upload_csv.sh</i> upload CSV files to a GitHub repository.
2. <strong>Kestra</strong>: CSV files now stored in the Github Release, will be orchestrated to GCP bucket and then ingested into <i>server_logs_data</i> Bigquery dataset.
  -->




<!-- ##  9. Log File Analysis Examples
- [Streaming process NASA web access logs on GCP](https://q15928.github.io/2019/06/10/nasa-log-analysis/)
- [Scalable Log Analytics with Apache Spark — A Comprehensive Case-Study](https://towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977)
- [Web Log Mining](https://medium.com/@dilshadakhan24/web-log-mining-association-rules-function-model-nasa-web-access-logs-c72eddc26bb4)


https://medium.com/@kazarmax/from-api-to-dashboard-building-an-end-to-end-etl-pipeline-with-azure-9b498daa2ef6


 -->




<!-- 

###  2.1. Export Logs Periodically with the [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli#install-the-heroku-cli)

1. Setup a Cron Job to Extract Server Logs via Heroku CLI.

2. Use bash script to clean the data and extract the data into CSV files.

    ```bash
    # make the script executable:
    chmod +x process_raw_txt.sh

    #  confirm if the file is executable
    ls -l process_raw_txt.sh

    # run the script
    bash process_raw_txt.sh
    ```

3. Load CSV files into Github Release automatically.

  1. Create a repo: <i>heroku_log_files</i>
  2. Create an initial release in your target Github repository - <i>heroku_log_files</i>:
      - GitHub > Releases > Create a new release
      - Choose a tag: <i>daily-upload</i> > Create new tag
      - Release title: <i>Daily CSV Upload</i>
      - Publish release
  3. Generate a GitHub Personal Access Token (PAT) - needed to authenticate when uploading files via script.
  - GitHub > Settings > Developer Settings
  - Personal access tokens > Fine-grained tockens > Generate new token
  - Token name: Heroku Releases Automation
  - Set expiration date
  - Choose repositories you want this token to access:
    - Select "Only select repositories" and pick the relevant repository.
  - Set Permissions under <i>Permissions</i>:
      - Actions: Read and write if you're triggering workflows.
      - Contents: Read and write (to upload files/releases).
      - Metadata: Read-only (to read repo info).
  - Generate and copy the Token.


  4. Write a script <i>upload_csv.sh</i> using the GitHub CLI (gh) to automate uploading the CSV files: 
  
  ```bash
  #!/bin/bash

  # set variables
  REPO="your-github-repo"
  TAG="daily-upload" # release tag where files will be uploaded
  CSV_FOLDER="/your-local-folder-with-csvs/"  # path to your local CSV files

  # create release if it doesn't exist
  if ! gh release view "$TAG" --repo "$REPO" > /dev/null 2>&1; then
    gh release create "$TAG" --repo "$REPO" --title "Daily CSV Upload" --notes "Automated daily upload"
  fi

  # upload CSV files to the release
  for file in "$CSV_FOLDER"/*.csv; do
    echo "Uploading $file to release $TAG..."
    gh release upload "$TAG" "$file" --repo "$REPO" --clobber
  done

  echo "Upload complete!"
  ```
  - make the script executable:
  ```bash
  chmod +x upload_csv.sh
  ```

  - Install GitHub CLI if not installed:
  ```bash
  sudo apt update && sudo apt install gh -y # Ubuntu
  ```
  - Authenticate GitHub CLI:
  ```bash
  echo YOUR_TOKEN_HERE | gh auth login --with-token
  ```
 -->



<!-- 
          ```yml
          version: 2

          sources:
              - name: staging
                database: gcp_project_id # replace with yours
                schema: log_files_data_all # same as your BigQuery dataset

                tables:
                    - name: daily_data # the partitioned table from your BigQuery dataset that consolidates all daily data
          ``` -->





<!-- - create categories_by_page.csv file in dbt cloud under the seeds folder
- use cat and then copy what’s on my terminal and paste the values into the all_pages.csv file
  ```bash
  cd to_the_folder_with_the_all_pages.csv
  cat categories_by_page.csv
  ``` -->
<!-- - add categories.sql to the models/core folder -->


<!-- https://github.com/EdidiongEsu/capital_bikeshare?tab=readme-ov-file
https://github.com/ankurchavda/streamify
https://github.com/hoe94/DTC_DE_FinalProject
https://github.com/MarcosMJD/ghcn-d -->